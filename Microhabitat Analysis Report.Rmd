---
title: "LCWMD Microhabitat Data Review"
author: "Curtis C. Bohlen"
date: "2/3/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

```{r results = FALSE, message=FALSE}
library(tidyverse)
library(readxl)

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())
```

```{r}
the_data <- read_excel('2020_habitat_data.xls', na = 'ND',
                       col_types = c('text', 'text', rep('numeric', 17))) %>%
  select(-Sum, -Count) %>%
  rename(Station = `Station ID`,
         Plot = `Plot ID`) %>%
  mutate(Plot_num = as.numeric(substr(Plot, 1, nchar(Plot)-1)),
         Subplot = substr(Plot, nchar(Plot), nchar(Plot))) %>%
  mutate(across(c(-Station, - Plot, - Plot_num, -Subplot),
                ~ replace_na(., 0))) %>%
  rename(Leaf_Litter = `Leaf Litter`)
```

```{r}
the_names = names(the_data[3:17])
long_data <- the_data %>%
  pivot_longer(c(-Station, - Plot, - Plot_num, -Subplot), 
               names_to = 'Microhabitat',
               values_to = 'Cover') %>%
  mutate(Microhabitat = factor(Microhabitat, levels = the_names)) %>%
  mutate(Reach = factor(Station, levels  = c('S05', 'S17'), 
                        labels =c('Reference', 'Restoration')))
rm(the_names)
```

# Introduction
A few years ago, LCWMD undertook an effort to improve habitat quality along a
portion of Long Creek.  Years of adjacent development activity had restricted
the floodplain, eroded the stream channel, and removed woody debris and other
roughness elements from the channel.  In what became perhaps the most ambitious
urban stream restoration in Maine history, LCWMD removed fill from the
floodplain, installed log jams and other structures in the stream, added gravel
and cobbles to the stream, and emplaced downed trees, rootwads, and wood in the
floodplain.

GZA Geoenvironmental Incorporated (GZA) has been monitoring the effects of
the restoration on stream habitat and the biota. As a portion of their 
monitoring, they conducted a study of relative abundance of different
microhabitats and substrate types in the restored stream reach and in a 
"reference reach" a short way upstream.  The study looked at visual estimates 
of percent cover of each of fifteen microhabitats and substrate types in 63 
quadrats (3 replicates at each of 21 random locations) in each stream reach.

In this report, I present results of reanalysis of the GZA data, along with some
commentary. For simplicity's sake, I ignore the hierarchical structure
(quadrats within locations) imposed by the sampling design.  A comprehensive
analysis should address that structure.  Such clustering of samples would be
expected to increase uncertainty in overall estimates of microhabitat abundance.
Results of this somewhat simplified analysis should be evaluated with that in
mind.

The report is generated by an R Markdown document. All figures and results are
calculated live from the data based on code in R Markdown. I have hidden the
code that generated the analysis to keep the focus on the meaning and ideas,
rather than implementation details.

You can download the original Markdown document that contains the R code from 
GitHub at:

(https://github.com/ccb60/LCWMD_2020_microhabitat.git)

## Goals
The goal of the following analysis is to estimate the relative area of each
microhabitat in each stream reach, and determine if the relative abundances are
similar or different between the restoration reach and the reference reach.

## Primary Analysis Approach
The best estimate of the relative area of each microhabitat can be obtained by 
be adding up the area of each microhabitat observed in each quadrat,
and dividing by the total area of all of the quadrats. That weighted sum
gives an estimate of the relative percent cover of each microhabitat in the
stream reach.

But that weighted sum is just the arithmetic mean of the observed percent cover
in all 63 quadrats (including zeros, where a particular microhabitat was not
observed in a quadrat).

Although we often mistrust the mean when dealing with skewed data, in this case,
the mean is the correct summary statistic for our purposes, so we need to work 
with it, rather than a median or other more resistant estimate of location.

# Calculate Estimated Percent Covers
We start by calculating by stream reach the number of plots in which each
microhabitat was observed, and the estimated percent cover of each microhabitat.
```{r}
pooled_data <- long_data %>%
  group_by(Microhabitat, Reach) %>%
  summarize( n = sum(Cover > 0),
             Cover = mean(Cover),
            .groups = 'drop')
pooled_data %>%
  mutate(Cover = round(Cover,2)) %>%
  pivot_wider(names_from  = Reach, values_from = c(n, Cover)) %>%
  knitr::kable()
```

## A Quick Check
We check to see whether the total percent cover in each reach is close to 100%.
If our math is right, each total should be 100% (within rounding error).
```{r}
tot <- pooled_data %>%
  group_by(Reach) %>%
  summarize(tot_cover = sum(Cover, na.rm = TRUE)) %>%
  pull(tot_cover)
names(tot) <- levels(pooled_data$Reach)
tot
rm(tot)
```

## Plot Results
It's easier to digest the results from a graphic, rather than a table.
```{r}
plt <- ggplot(pooled_data, aes(Microhabitat, Cover)) +
  geom_col(aes(fill = Reach), position = 'dodge')  +

  xlab('') +
  ylab('Percent Cover') +
  
  theme_cbep(base_size = 9) +
  theme(axis.text.x = element_text(angle = 90, vjust = .25, hjust = 1))
plt
```

Visually, what jumps out are:  

*  High levels of Muck, Gravel, and Cobble in the Restoration Reach  

*  High levels of Silt and Sand in the Reference Reach.

# Testing for "Significance"
How can we be sure those apparent "differences" are meaningful, and equally 
important, that less visually less impressive differences are NOT meaningful?

## Can We Use t Tests? 
We just calculated Percent Cover of each Microhabitat in each reach as the 
average of 63 observations from each stream reach. Comparing two means is 
what t tests are for.

Unfortunately, t tests are not ideal for several reasons. Most importantly, the
observed data are far from normally distributed, and the sample size is only
moderate. The t statistic is based on asymptotic behavior of a sum or mean drawn
from a population with an unknown standard deviation, which therefore must be
estimated from the data. Convergence on the (symmetrical) t distribution is slow
when you start with a highly skewed distribution (as here).

Another, more subtle point is that the different microhabitat types are not 
independent, since they must add up to 100% of the habitat. Comparing each
microhabitat separately is likely to overstate statistical significance 
(although by only a small bit here).

What that all means is a t test may give relatively poor control of Type 1
error. The nominal "P value" may not be especially trustworthy. Because of the
central limit theorem (sums of random variables converge to a normal
distribution), the larger the sample size, the less the distribution of the
underlying data matters. Since our sample sizes are fairly large (n = 63),
things may work out. If we don't take the nominal p value too seriously, we may
be O.K.

## Try a t Test Anyway
Despite my reservations, I went ahead and calculated nominal p values from a
standard t test for comparing two samples with unequal variances (Welch's t
test).  I used two tailed t tests because I am interested in checking if
there are any differences between the two samples.  I don't care which is larger 
and which is smaller.
```{r}
pull_d_and_p <- function(d) {
  tst <- t.test(Cover ~ Reach, data = d)
  vals <- tst$estimate
  difs = round(vals[[1]] - vals[[2]],2)
  return (c(difference = difs,
                p.value = round(unname(tst$p.value),4)))
}

a <- by(long_data, long_data$Microhabitat,
       pull_d_and_p)
b <- t(do.call("cbind",a))
b

res1 <- b[,'p.value']
rm(a,b)
```

## Resampling:  An Alternative to the t Test
The most straight forward approach to comparing two means from "badly behaved" 
distributions is probably to use a resampling test.  This addresses the skewed
nature of the data, but does not address the fact that the cover of all
microhabitats us sum to 100%.

Under the null hypothesis of no difference in microhabitat distribution, the
distribution of possible observations of percent cover from the two reaches
should be the same. We can readily simulate a draw from two data distributions
that matches that description, without relying on any assumptions about the
nature of the underlying distribution by sampling repeatedly from the actual
observations.

For each microhabitat, we can draw two samples of 63 from all the observations
of that microhabitat (including the zeros) from both stream reaches. That random
sample is an (estimated) example of what we might expect to observe under the
null hypothesis.  We can then then calculate the difference between these two
"random" samples.  Since the two samples were drawn from the same data, they
will usually be similar, but sometimes the samples will be quite different

We do that many times, and count up how many times we see differences between
the two simulated samples that are as large as the difference we observed in the
field. The frequency of extreme observations provides an estimate of the "p
value" of the observed difference between the two stream reaches based on a
resampling or "bootstrap" test comparing the to populations.

I wrapped the resampling logic into a function, and called the function for each
of the different microhabitats.  In the function, I draw 10,000 bootstrap
samples. Since this is based on a a random process, the estimated p values will
change slightly each time the code is run, but with a resample of 10,000, it
should not vary much.

```{r create_fxn}
the_test <- function(my_data) {
  # How big a difference did we actually see?
  observed <- my_data %>%
    group_by(Reach) %>%
    summarize(Cover = mean(Cover)) %>%
    pull(Cover)
  threshold = abs(observed[1] - observed[2])
  
  # Run the bootstrap
  difs <- vector('numeric', 10000)
  for (samp in 1:10000) {
    ref  <- sample(my_data$Cover, 21*3, replace = TRUE)
    rest <- sample(my_data$Cover, 21*3, replace = TRUE)
    dif = mean(ref) - mean(rest)
    difs[samp] <- dif
  }
    
    return(list(difference = threshold,
                p.value= sum(difs > threshold | difs < -threshold) / 10000))
}
```

```{r run_resamples}
micros <- levels(long_data$Microhabitat) 
results <- vector('list', length(micros))
for (m in 1:length(micros)) {
  r <- the_test(long_data[long_data$Microhabitat == micros[[m]],])
  results[[m]] <- r
}
names(results) <- micros
```

## Resampling Results
```{r pull_resample_results}
a <- do.call("rbind", results)
res2 <- unlist(a[,'p.value'])
a
rm(a)
```

## Compare Two Tests
It is instructive to compare the p value estimates for the two different tests.
The p value is a property of the statistical model, not only of the data, so 
different analytic methods give different p values. Here, the two 
tests both test differences between observed means.  Both p values estimate the
probability that the observed difference between the two means would arise by
chance alone, if there were no real difference in abundance of the specific 
microhabitat.
```{r}
tbl <- cbind(res1, res2)
colnames(tbl) <- c('t_test', 'Resampling')
tbl
```
Qualitatively, the results are similar.  In only one case (Bedrock) does
the choice of test change the qualitative conclusions we would draw.  That gives
me some confidence that the conclusions are robust to the statistical approach 
used.

We can conclude (based on the suggested "liberal" critical p value of 10%) that
differences in microhabitat abundance for the following microhabitats 
are probably meaningful:

*  Muck  
*  Silt  
*  Sand  
*  Gravel  
*  Cobble  
*  Bedrock (by one test and not the other)  
*  Aquatic Bed  
*  Fill Garbage  

# Modified Graphics
## Add a Marker of Statistical Significance
It is nice to add some sort of a marker to the graphic showing which
comparisons are statistically robust.
```{r}
should_annot <- res2 < 0.1

x = 1:15
xlocs = if_else(should_annot, x, NA_integer_)

plt  +
annotate(geom = 'point', xlocs, y = 0, shape = 22, size = 2.5, fill = 'gray80') +
annotate('text', x = 7, y = 28, hjust = 0, size = 3,
           label = 'Gray boxes mark significant\ndifferences between reaches')
```

## Add Confidence Intervals
A graphical alternative is to add standard deviation, standard error, or
confidence intervals to the graphic.  I would not use standard deviation, as the
error distribution is likely to be asymmetrical, especially for rarely seen 
habitats. A standard error of the mean (or symmetric confidence intervals
derived from the standard error) hold the same risk.  My preference is to go with bootstrapped confidence intervals.

Bootstrapped confidence intervals (in their simplest form) resample from the 
observed data and calculate the mean of that sample.  Do that over and over 
again and you can use the distribution of those resampled means to estimate the
likely outer limits of what you would expect to observe if you repeated the
study.

I calculated 90% confidence intervals (matching the proposed 10% P value) via a 
simple bootstrap model, built into another small function.  The intervals are 
based on 10,000 bootstrap samples. The specific values are the result of 
resampling, so the exact values will differ in the third or fourth decimal
place each time the code is run.

### Calculate Confidence Intervals
```{r create_CI_fxn}
the_CI_90<- function(my_data) {
  # Run the bootstrap
  mns <- vector('numeric', 10000)
  for (samp in 1:10000) {
    s  <- sample(my_data$Cover, 21*3, replace = TRUE)
    m <- mean(s, na.rm = TRUE)
    mns[samp] <- m
  }

    return(list(Upper.CL = unname(quantile(mns, 0.95)),
                Lower.CL = unname(quantile(mns, 0.05))))
}
```

```{r run_confidence_intervals}
micros <- levels(long_data$Microhabitat)
reaches <- c('Reference', 'Restoration')
results <- vector('numeric', length(micros) * length(reaches)* 2)
results <- array(results,
                    dim = c(2, 15, 2),
                 dimnames = list( c('Upper.CL', 'Lower.CL'), micros, reaches))
for (m in 1:length(micros)) {
  for (rch in 1:length(reaches)) {
    r <- long_data %>%
      filter(Microhabitat == micros[m],
             Reach == reaches[rch]) %>%
      the_CI_90()
    results[1, m, rch] <- round(r[[1]],3)
    results[2, m, rch] <- round(r[[2]],3)
  }
}
cat('Reference Reach Bootstrapped 90% Confidence Intervals\n')
t(results[,,'Reference'])
cat('\n\nRestoration Reach Bootstrapped 90% Confidence Intervals\n')
t(results[,,'Restoration'])
```

```{r create_CI_tibble}
  # There must be a more efficient way to do this....
r1 = t(results[,,'Reference'])
t1 <- as_tibble(r1) %>%
  mutate(Reach = 'Reference',
         Microhabitat = micros)
r2 <- t(results[,,'Restoration'])
t2 <- as_tibble(r2) %>%
  mutate(Reach = 'Restoration',
         Microhabitat = micros)
results_tibble <- bind_rows(t1, t2)
results_tibble <- results_tibble
 #rm(results, r1, t1, r2, t2)
```

### Create the Graphic
We can also add those confidence intervals to the previous graphic.
```{r}
tmp <- pooled_data %>% 
  left_join(results_tibble, by = c("Microhabitat", "Reach")) %>%
  mutate(Microhabitat = factor(Microhabitat,
                               levels = levels(long_data$Microhabitat)))
  
plt2 <-  ggplot(tmp, aes(Microhabitat, Cover)) +
  geom_col(aes(fill = Reach), position = 'dodge')  +
  geom_linerange(mapping = aes(ymin = Lower.CL,
                               ymax = Upper.CL,
                               group = Reach),
                 position = position_dodge(width = 0.9)) +
  xlab('') +
  ylab('Percent Cover') +
  
  theme_cbep(base_size = 9) +
  theme(axis.text.x = element_text(angle = 90, vjust = .25, hjust = 1)) +
  
  ggtitle(' Estimates and Bootstrapped 90% Confidence Intervals')
  
  

plt2
```



# An Alternative Approach
Practically, there are a few other approaches to testing statistical
significance that one might apply. Each method asks and answers a slightly
different questions about the data. 

The simplest practical alternative is probably to use a contingency table to
test frequency of observing habitat types, and test if frequencies are different
between the two stream reaches. (After that I might consider hierarchical
models, but that is overkill here.)

A contingency table does two things differently.  First, it only determines if
microhabitat frequencies *overall* are different (it does not tell you WHICH
frequencies are different).  And second, it only looks at how many times you
observed each microhabitat in the 63 quadrats.  It does not consider what the
percent cover within each quadrat was.

In that sense it's a less informative test, but it is complementary 
to the prior approaches, so it provides another way to check whether our
qualitative conclusions depend on how we conduct the analysis.

## Contingency Table Analysis
Contingency tables are notoriously unhappy with cells with an "expected" number
under the null hypothesis under 5 or so. As a result, we should pool the less
common microhabitat types before analysis

We pool "Rare" habitats -- those with fewer than 10 observations.  

*  ClayPieces  

*  HardClayBottom  

*  Boulder  

*  Bedrock  

*  Fern  

*  Fill_Garbage  

and end up with the following contingency table:
```{r}
ct_data <- long_data %>%
  filter(Cover > 0) %>%
  mutate(Microhabitat = recode (Microhabitat,
                                ClayPieces= "Rare",
                                HardClayBottom = "Rare",
                                Boulder = "Rare",
                                Bedrock = "Rare",
                                Fern = "Rare",
                                Fill_Garbage = "Rare"))
```

```{r}
(mytab <- xtabs(~ Microhabitat + Reach, data = ct_data))
```

It is worth pointing out that some of the microhabitats that showed robust
differences in the t test and resampling analyses  show little difference in the
number of plots in which they occurred.  Look at Sand and Silt, for example.

Sand and Silt may be found (nearly) everywhere, but the abundance within plots
is different between the two stream reaches.  The different statistical 
approaches are sensitive to different aspects of the spatial arrangement of
microhabitats.

```{r}
chisq.test(mytab)
```
So the relative abundance of different of microhabitats differs between the two 
stream reaches. It's always nice when multiple statistical models lead you to the
same conclusions.


# Final Notes  
1.  I was surprised that the results of the t-test were so similar to the
    results of the resampling test.  That suggests it may not be unreasonable 
    to use a t test in future comparisons (interpreting results with suitable 
    humility and care). Relatively good performance of the t test is based on 
    having a large sample size. If the number of quadrats drops, the t test will
    not perform as well.
    
2.  Analyses could be repeated, taking into account the hierarchical
    sampling design.  The effect would probably be to widen confidence
    intervals and reduce apparent statistical significance of comparisons. The 
    analysis would also provide some insight into the spatial distribution
    of microhabitats within each stream reach. But hierarchical analysis 
    significantly complicates things, for relatively small improvement in 
    information. I would not expect the more complex analysis to alter the
    qualitative conclusions form this analysis.  The added effort is probably 
    only justified if greater precision is needed, for scientific publication, 
    to address controversy, or to respond to a specific management question.
    
3.  The purpose of collecting the microhabitat data is to understand
    how abundance of microhabitats changed following restoration, and how the 
    distribution of microhabitats evolves post-restoration.  That makes 
    comparison of change over time within each stream reach arguably more
    important than a comparison of microhabitat abundances between the two 
    reaches. The analysis just presented offers a preview of issues (and
    some possible solutions) for a change over time analysis. Correct analysis
    will also depend on details of the sampling design, and in particular, how 
    sampling locations were selected each year.
    

